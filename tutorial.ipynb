{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will cover these topics:\n",
    "\n",
    "1- A brief overview of available libraries for deeplearning.\n",
    "\n",
    "2- Basic theano concepts and functionalities including computation graphs, shared variables, theano function, scan, etc.\n",
    "\n",
    "3- Simple examples for Logistic Regression, Neural Networks and Recurrent Neural Networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of different Packages:\n",
    "<img src=\"overview.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computation graph is the core idea behind Theano's architecture. All the functions that the user wants to give to theano must be declared in a computation graph first. The main reason is that it allows Theano to compute analytical derivation for these graphs that later will be used for optimization. Having computation graph also provides a convenient way for code optimization before that the actual process starts, similar to compiler based languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Output is ', array(900, dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "\n",
    "# Symbolic Variables \n",
    "x= T.iscalar('x')\n",
    "y= T.iscalar('y')\n",
    "z= T.iscalar('z')\n",
    "a= T.iscalar('a')\n",
    "\n",
    "# Construct computation Graph\n",
    "x= y+z\n",
    "a= x**2\n",
    "\n",
    "# Compile the function\n",
    "f= theano.function([y,z], a)\n",
    "\n",
    "# Create real values and evaluate the function\n",
    "y_real= 10\n",
    "z_real= 20\n",
    "\n",
    "a_real= f(y_real, z_real)\n",
    "\n",
    "print('Output is ', a_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computation Graph Exercise:\n",
    "Create a function that computes the logistic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learned Concepts:\n",
    "1- Shared Variables \n",
    "2- Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('State = ', array(0.0))\n",
      "('State = ', array(5.0))\n",
      "('State = ', array(10.0))\n",
      "('State = ', array(15.0))\n"
     ]
    }
   ],
   "source": [
    "state= theano.shared(0.0)\n",
    "inc= T.iscalar('inc')\n",
    "accumulator= theano.function([inc], state, updates= [( state, state+inc)])\n",
    "incc= 5\n",
    "print('State = ', accumulator(incc))\n",
    "print('State = ', accumulator(incc))\n",
    "print('State = ', accumulator(incc))\n",
    "print('State = ', accumulator(incc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression Tutorial:\n",
    "\n",
    "This uses the shared variables, copmmutation graph, updates concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial model:\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "N = 400                                   # training sample size\n",
    "feats = 784                               # number of input variables\n",
    "\n",
    "# generate a dataset: D = (input_values, target_class)\n",
    "D = (np.random.randn(N, feats), np.random.randint(size=N, low=0, high=2))\n",
    "training_steps = 10000\n",
    "\n",
    "# Declare Theano symbolic variables\n",
    "x = T.dmatrix(\"x\")\n",
    "y = T.dvector(\"y\")\n",
    "\n",
    "# initialize the weight vector w randomly\n",
    "#\n",
    "# this and the following bias variable b\n",
    "# are shared so they keep their values\n",
    "# between training iterations (updates)\n",
    "w = theano.shared(np.random.randn(feats), name=\"w\")\n",
    "\n",
    "# initialize the bias term\n",
    "b = theano.shared(0., name=\"b\")\n",
    "\n",
    "print(\"Initial model:\")\n",
    "#print(w.get_value())\n",
    "print(b.get_value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct Theano expression graph\n",
    "p_1 = 1 / (1 + T.exp(-T.dot(x, w) - b))   # Probability that target = 1\n",
    "prediction = p_1 > 0.5                    # The prediction thresholded\n",
    "cent = -y * T.log(p_1) - (1-y) * T.log(1-p_1) # Cross-entropy loss function\n",
    "cost = cent.mean() + 0.01 * (w ** 2).sum()# The cost to minimize\n",
    "\n",
    "gw, gb = T.grad(cost, [w, b])             # Compute the gradient of the cost\n",
    "                                          # w.r.t weight vector w and\n",
    "                                          # bias term b\n",
    "                                          # (we shall return to this in a\n",
    "                                          # following section of this tutorial)\n",
    "\n",
    "# Compile\n",
    "train = theano.function(\n",
    "          inputs= [x,y],\n",
    "          outputs= [prediction, cent],\n",
    "          updates=((w, w - 0.1 * gw), (b, b - 0.1 * gb)))\n",
    "predict = theano.function(inputs=[x], outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model:\n",
      "0.00507681637201\n",
      "target values for D:\n",
      "[0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 1 0 1 1 1 0 1 0\n",
      " 0 1 1 1 0 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 1 1 0 0 0 1 1 0 1 0\n",
      " 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1\n",
      " 0 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1 1 1\n",
      " 1 1 0 0 0 1 1 0 1 1 0 1 1 0 0 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 0 1 0 0 1 1 0\n",
      " 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 1 0 1 1 1\n",
      " 0 1 1 1 0 0 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1\n",
      " 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 0\n",
      " 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 0 0 1 1 1 0 0 1\n",
      " 1 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 1 0 1 1 0 1 1 1\n",
      " 0 1 1 1 0 0 0 0 0 0 0 0 1 1 0 1 1 1 1 0 0 0 0 0 0 1 1 1 1 1]\n",
      "prediction on D:\n",
      "[0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 1 0 1 1 1 0 1 0\n",
      " 0 1 1 1 0 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 1 1 0 0 0 1 1 0 1 0\n",
      " 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1\n",
      " 0 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1 1 1\n",
      " 1 1 0 0 0 1 1 0 1 1 0 1 1 0 0 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 0 1 0 0 1 1 0\n",
      " 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 1 0 1 1 1\n",
      " 0 1 1 1 0 0 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1\n",
      " 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 0\n",
      " 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 0 0 1 1 1 0 0 1\n",
      " 1 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 1 0 1 1 0 1 1 1\n",
      " 0 1 1 1 0 0 0 0 0 0 0 0 1 1 0 1 1 1 1 0 0 0 0 0 0 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for i in range(training_steps):\n",
    "    pred, err = train(D[0], D[1])\n",
    "\n",
    "print(\"Final model:\")\n",
    "#print(w.get_value())\n",
    "print(b.get_value())\n",
    "print(\"target values for D:\")\n",
    "print(D[1])\n",
    "print(\"prediction on D:\")\n",
    "print(predict(D[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise Write your own simple neural network (Fully Connected one). \n",
    "\n",
    "It has two layers: \n",
    "\n",
    "Layer1: y1= tanh(w1*x+b1)\n",
    "\n",
    "Layer2: y2= w2*y1+b2\n",
    "\n",
    "Layer3: y3= sigmoid(y2)\n",
    "\n",
    "Cost is cross entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1- Create shared Variables for weights and initialize them to random + symbolic variables\n",
    "N, D, H, C= 10, 1000, 100, 1\n",
    "\n",
    "x= T.matrix('x')\n",
    "y= T.vector('y', dtype= 'int64')\n",
    "\n",
    "w1= theano.shared(np.random.randn(D, H),name='w1')\n",
    "w2= theano.shared(np.random.randn(H, C),name='w2')\n",
    "b1= theano.shared(np.zeros((H,)), name= 'b1')\n",
    "b2= theano.shared(np.zeros((C,)), name= 'b2')\n",
    "\n",
    "# 2- Forward Pass Computation Graph\n",
    "# Write Code Here\n",
    "\n",
    "# 3- Backward Pass Compute Gradients\n",
    "# Write Code Here\n",
    "\n",
    "# 4- Define your train function with the proper updates using sgd\n",
    "# Write Code Here\n",
    "\n",
    "# 5- Evaluate your train function\n",
    "xx= np.random.randn(N, D)\n",
    "yy= np.random.randint(size= N, low= 0.0, high= C+1)\n",
    "\n",
    "# Write Code Here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scan in theano:\n",
    "\n",
    "Scan function works as a loop for computational graphs. Similar to regular loops it has an iterable  object to iterate through, a stopping criteria and output. It also can have states to get updated at each iteration and initial values for those state.The difference to regular loop is that in scan everything is in computation graph and therefore derivable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   1.   4.   9.  16.  25.  36.  49.  64.  81.]\n"
     ]
    }
   ],
   "source": [
    "k = T.iscalar(\"k\")\n",
    "A = T.vector(\"A\")\n",
    "\n",
    "def inner_fct(prior_result, B):\n",
    "    return prior_result * B\n",
    "\n",
    "# Symbolic description of the result\n",
    "result, updates = theano.scan(fn=inner_fct,\n",
    "                            outputs_info=T.ones_like(A),\n",
    "                            non_sequences=A, n_steps=k)\n",
    "\n",
    "# Scan has provided us with A ** 1 through A ** k.  Keep only the last\n",
    "# value. Scan notices this and does not waste memory saving them.\n",
    "final_result = result[-1]\n",
    "\n",
    "power = theano.function(inputs=[A, k], outputs=final_result,\n",
    "                      updates=updates)\n",
    "\n",
    "print(power(range(10), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple example of Recurrent Neural Networks (RNN):\n",
    "\n",
    "Theano is one the most advantageous libraries when it comes to RNNs. Mostly thanks to the Scan function that facilitates recursion. A very simple example of RNN (You can say THE simplest) is brought here. Even though it is a very straight forward network implementing it in other frameworks (Caffe for instance) is quite troublesome.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter2000 = 0.0550554811996\n",
      "loss at iter4000 = 0.0118183404572\n",
      "loss at iter6000 = 0.00806469615169\n",
      "loss at iter8000 = 0.00638707483387\n",
      "loss at iter10000 = 0.00496667789957\n",
      "loss at iter12000 = 0.00404027904459\n",
      "loss at iter14000 = 0.00396884035201\n",
      "loss at iter16000 = 0.00321019609914\n",
      "loss at iter18000 = 0.00275164154232\n",
      "loss at iter20000 = 0.00273580186183\n",
      "loss at iter22000 = 0.00237600998375\n",
      "loss at iter24000 = 0.00226156587989\n",
      "loss at iter26000 = 0.00212060954238\n",
      "loss at iter28000 = 0.0020442929394\n",
      "loss at iter30000 = 0.00209396525777\n",
      "loss at iter32000 = 0.00194990690074\n",
      "loss at iter34000 = 0.00183692264311\n",
      "loss at iter36000 = 0.0017624415877\n",
      "loss at iter38000 = 0.00162449743692\n",
      "loss at iter40000 = 0.00165503710862\n",
      "loss at iter42000 = 0.00143846916362\n",
      "loss at iter44000 = 0.00147361060614\n",
      "loss at iter46000 = 0.00144785313334\n",
      "loss at iter48000 = 0.00138829674885\n",
      "loss at iter50000 = 0.00130817150485\n",
      "loss at iter52000 = 0.00130887236299\n",
      "loss at iter54000 = 0.00123675782399\n",
      "loss at iter56000 = 0.00137356988369\n",
      "loss at iter58000 = 0.0011942169543\n",
      "loss at iter60000 = 0.00124800719912\n",
      "loss at iter62000 = 0.00118602911442\n",
      "loss at iter64000 = 0.00117674104929\n",
      "loss at iter66000 = 0.0012019086994\n",
      "loss at iter68000 = 0.00113527366683\n",
      "loss at iter70000 = 0.00108541377725\n",
      "loss at iter72000 = 0.00108366303513\n",
      "loss at iter74000 = 0.00112328420451\n",
      "loss at iter76000 = 0.00113465409356\n",
      "loss at iter78000 = 0.00106431148532\n",
      "loss at iter80000 = 0.00108163335627\n",
      "loss at iter82000 = 0.00103753252588\n",
      "loss at iter84000 = 0.001098142798\n",
      "loss at iter86000 = 0.00108663277164\n",
      "loss at iter88000 = 0.000920669333733\n",
      "loss at iter90000 = 0.00100727140878\n",
      "loss at iter92000 = 0.00102373437638\n",
      "loss at iter94000 = 0.00102648007238\n",
      "loss at iter96000 = 0.000881114430919\n",
      "loss at iter98000 = 0.000862478161425\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Implementing a RNN netowrk with single scalar hidden state.\n",
    "It takes a vector of floats as input and outputs a single float.\n",
    "Data (input vector and output lable) is generated on the fly. \n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "\n",
    "class network():\n",
    "    \"\"\"\n",
    "    Encapsulate the networks architecture (layers), function compilation and the train function. \n",
    "    \"\"\"\n",
    "    def __init__ (self):\n",
    "        \"\"\"\n",
    "        Initilizing the network with a fixed two layers(one RNN layer and one Euclidian loss layer.) structure.\n",
    "        Generates theano functions for forward pass and update. \n",
    "        Simple Stochastic Gradient Descent for optimization.\n",
    "        \"\"\"\n",
    "        #Networks learnable parameters\n",
    "        self.params = {}\n",
    "        \n",
    "        #learning rate for SGD\n",
    "        #Play with for fast and smooth convergance (specifically try 0.01 and 0.005 and 0.001). \n",
    "        #Try to make it adaptive.\n",
    "        lr = np.array(0.005, dtype=\"float32\")\n",
    "        \n",
    "        #Input vector and label\n",
    "        self.x = T.vector('x')\n",
    "        self.y = T.scalar('y')\n",
    "        \n",
    "        #Creating the network                                                                                                                                                    \n",
    "        self.RNN_out = self.RNN(self.x)\n",
    "        self.loss = self.euc_loss(self.RNN(self.x), self.y)\n",
    "        \n",
    "        #Computing gradients w.r.t network params.\n",
    "        self.grads = T.grad(self.loss, wrt = list(self.params.values()) )\n",
    "        \n",
    "        #SGD optimization\n",
    "        gshared = [theano.shared(p.get_value() * np.array(0.0, dtype=\"float32\"), name='%s_grad' % k) for k, p in self.params.items()]\n",
    "        gsup = [(gs, g) for gs, g in zip(gshared, self.grads)]\n",
    "        pup = [(param, param - lr*g) for param, g in zip(self.params.values(), gshared)]\n",
    "        \n",
    "        #Forward pass fucntion for testing\n",
    "        self.f_forward = theano.function([self.x], outputs=[self.RNN_out])\n",
    "        \n",
    "        #Train function\n",
    "        self.train = theano.function([self.x, self.y], outputs=[self.RNN_out, self.loss], updates=gsup + pup)\n",
    "    \n",
    "        \n",
    "    def RNN(self, X):\n",
    "        \"\"\"\n",
    "        A simple one node RNN. There are many good references on RNN. This is an interesting one : \n",
    "        http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "        \"\"\"\n",
    "        \n",
    "        #defining shared variables to be used as weights\n",
    "        self.params['W'] = theano.shared(value=np.array(np.random.rand(), dtype=\"float32\"), name= 'W')\n",
    "        self.params['U'] = theano.shared(value=np.array(np.random.rand(), dtype=\"float32\"), name= 'U')\n",
    "        self.params['b'] = theano.shared(value=np.array(np.random.rand(), dtype=\"float32\"), name= 'b')\n",
    "        self.params['Wo'] = theano.shared(value=np.array(np.random.rand(), dtype=\"float32\"), name= 'Wo')\n",
    "        \n",
    "        #stopping criteria\n",
    "        n_steps = X.shape[0]\n",
    "        \n",
    "        #RNN model\n",
    "        def step(x, _h): \n",
    "            h = T.tanh(self.params['W']*_h + self.params['U']*x + self.params['b'])\n",
    "            y = self.params['Wo'] * h\n",
    "            return y, h\n",
    "            \n",
    "        #Scan function that carries out recurssion\n",
    "        results, update = theano.scan(step, sequences = X, outputs_info =[None,  np.array(0.0, dtype=\"float32\")], n_steps = n_steps)\n",
    "        return results[0][-1]\n",
    "            \n",
    "\n",
    "    def euc_loss(self, inp, label):\n",
    "        \"\"\"\n",
    "        Euclidian loss function\n",
    "        \"\"\"\n",
    "        return (inp-label)**2\n",
    "\n",
    "def dummy(x):\n",
    "    \"\"\"\n",
    "    Dummy function for generating labels given x vector.\n",
    "    Change it to other fun stuff.\n",
    "    \"\"\"\n",
    "    return x.sum()\n",
    "\n",
    "#Initializing the netowrk object\n",
    "net = network()\n",
    "\n",
    "#Maximum number of iterations. Its optimum value is closely related to the learning rate.\n",
    "max_iter = 100000\n",
    "disp_freq = 2000\n",
    "\n",
    "loss = np.zeros(disp_freq)\n",
    "for i in range(1,max_iter):\n",
    "    x = np.array(np.random.rand(2), dtype=\"float32\")\n",
    "    y = dummy(x)\n",
    "    loss[i%disp_freq] = net.train(x,y)[1]\n",
    "    if i%disp_freq == 0:\n",
    "        print (\"loss at iter%s = %s\"%(i,loss.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
